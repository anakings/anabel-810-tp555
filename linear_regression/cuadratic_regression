import numpy as np

N=100

x = 2 * np.random.rand(N, 1)
y = 4 + 3 * x + np.random.randn(N, 1)

# Solve by applying the least-Squares method.
# We use the inv() function from NumPyâ€™s Linear Algebra module (np.linalg) to compute the inverse of a matrix.
# We use dot() method for matrix multiplication.
X_b = np.c_[np.ones((N, 1)), x] # add x0 = 1 to each instance #tiene que ser una matriz no singular por lo q no puede tener un valor igual a 0
a_optimum = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

print('a0: ' + str(a_optimum[0][0]))
print('a1: ' + str(a_optimum[1][0]))

# The equivalent solution using the Scikit-Learn library is given below
# Import the linear regression module form the library.
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression().fit(x, y) # instantiate it.
#lin_reg.fit(x, y)

print('a2: ' + str(lin_reg.intercept_[0])) # Value that crosses the y-axis when all features are equal to 0.
print('a3: ' + str(lin_reg.coef_[0][0])) # parameters associated with the features.
