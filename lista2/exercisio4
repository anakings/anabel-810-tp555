Entre os algoritmos baseados no gradiente descendente o que mais rapidamente chega à vizinhança da solução ótima é o algoritmo estocástico. Deles o que realmente converge é o batch. Para que o algoritmo estocástico e o mini-batch também convirjam deve usar uma boa
estratégia para modificação do passo de aprendizagem, que o passo de aprendizagem começa com grandes valores e depois diminue cada vez mais permitindo que o algoritmo se estabiliza no mínimo global.
