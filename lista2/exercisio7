import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
import math

"""
Hypothesis Function
"""
def hypothesis(X, A0, A1, A2=None, A3=None, A4=None):
    h = A0 + A1*X
    if A4 != None:
        h = h + A2*X + A3*X + A4*X
    elif A3 != None:
        h = h + A2*X + A3*X
    elif A2 != None:
        h = h + A2*X
    return h
            
"""
Calculate closed-form solution using the normal equation.
"""
def calculateClosedFormSolution(X, y, M):
    # Closed-form solution.
    a = np.linalg.pinv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y))
    if X.shape[1] == 5:
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0], A3=a[3, 0], A4=a[4, 0])
    elif X.shape[1] == 4:
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0], A3=a[3, 0])
    elif X.shape[1] == 3:
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0])
    else:
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0])
    Joptimum = (1/M)*np.sum(np.power((y - h), 2))
    
    return Joptimum, a

"""
Batch gradient descent solution.
"""
def batchGradientDescent(alpha, n_iterations, X, y):
    # Random initialization of parameters.
    a = np.zeros((X.shape[1],1))
    # Create vector for parameter history.
    a_hist = np.zeros((X.shape[1], n_iterations+1))
   
    Jgd = np.zeros(n_iterations+1)
    if X.shape[1] == 5:
        a[0] = -10
        a[1] = -10
        a[2] = -10
        a[3] = -10
        a[4] = -10
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0], A3=a[3, 0], A4=a[4, 0])
        a_hist[0, 0] = a[0]
        a_hist[1, 0] = a[1]
        a_hist[2, 0] = a[2]
        a_hist[3, 0] = a[3]
        a_hist[4, 0] = a[4]
    elif X.shape[1] == 4:
        a[0] = -10
        a[1] = -10
        a[2] = -10
        a[3] = -10
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0], A3=a[3, 0])
        a_hist[0, 0] = a[0]
        a_hist[1, 0] = a[1]
        a_hist[2, 0] = a[2]
        a_hist[3, 0] = a[3]
    elif X.shape[1] == 3:
        a[0] = -10
        a[1] = -10
        a[2] = -10
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0], A2=a[2, 0])
        a_hist[0, 0] = a[0]
        a_hist[1, 0] = a[1]
        a_hist[2, 0] = a[2]
    else:
        a[0] = -10
        a[1] = -10
        h = hypothesis(X, A0=a[0, 0], A1=a[1, 0])
        a_hist[0, 0] = a[0]
        a_hist[1, 0] = a[1]
    Jgd[0] = (1/M)*np.sum(np.power(y - h, 2))
    
    # Batch gradient-descent loop.
    for iteration in range(n_iterations):
        if X.shape[1] == 5:
            h = hypothesis(X, a_hist[0, iteration], a_hist[1, iteration], a_hist[2, iteration], a_hist[3, iteration], a_hist[4, iteration])
            gradients = -2/M * X_b.T.dot(y - h)
            a = a - alpha * gradients
            a_hist[0, iteration+1] = a[0, 0]
            a_hist[1, iteration+1] = a[1, 0]
            a_hist[2, iteration+1] = a[2, 0]
            a_hist[3, iteration+1] = a[3, 0]
            a_hist[4, iteration+1] = a[4, 0]
            h = hypothesis(X, a_hist[0, iteration+1], a_hist[1, iteration+1], a_hist[2, iteration+1], a_hist[3, iteration+1], a_hist[4, iteration+1])
        elif X.shape[1] == 4:
            h = hypothesis(X, a_hist[0, iteration], a_hist[1, iteration], a_hist[2, iteration], a_hist[3, iteration])
            gradients = -2/M * X_b.T.dot(y - h)
            a = a - alpha * gradients
            a_hist[0, iteration+1] = a[0, 0]
            a_hist[1, iteration+1] = a[1, 0]
            a_hist[2, iteration+1] = a[2, 0]
            a_hist[3, iteration+1] = a[3, 0]
            h = hypothesis(X, a_hist[0, iteration+1], a_hist[1, iteration+1], a_hist[2, iteration+1], a_hist[3, iteration+1])
        elif X.shape[1] == 3:
            h = hypothesis(X, a_hist[0, iteration], a_hist[1, iteration], a_hist[2, iteration])
            gradients = -2/M * X_b.T.dot(y - h)
            a = a - alpha * gradients
            a_hist[0, iteration+1] = a[0, 0]
            a_hist[1, iteration+1] = a[1, 0]
            a_hist[2, iteration+1] = a[2, 0]
            h = hypothesis(X, a_hist[0, iteration+1], a_hist[1, iteration+1], a_hist[2, iteration+1])
        else:
            h = hypothesis(X, a_hist[0, iteration], a_hist[1, iteration])
            gradients = -2/M * X_b.T.dot(y - h)
            a = a - alpha * gradients
            a_hist[0, iteration+1] = a[0, 0]
            a_hist[1, iteration+1] = a[1, 0]
            h = hypothesis(X, a_hist[0, iteration+1], a_hist[1, iteration+1])
        Jgd[iteration+1] = (1/M)*np.sum(np.power(y - h, 2))
        
    return a, a_hist, Jgd

def plotErrorVsIteration(Jgd, iteration):
    fig = plt.figure(figsize=(5, 5))
    plt.plot(np.arange(0, iteration), Jgd[0:iteration])
    plt.xlim((0, iteration))
    plt.yscale('log')
    plt.xlabel('Iteration')
    plt.ylabel('$J_e$')
    plt.title('Error vs. Iteration number')
    plt.show()

    
## --------------------------------------------------------
df = pd.read_csv('training.csv', header=None)
X1 = df[0].to_numpy()
X = np.zeros(shape=(y1.shape[0],1))
for i, value in enumerate(X1):
    X[i,0] = value
y1 = df[1].to_numpy()
y = np.zeros(shape=(y1.shape[0],1))
for i, value in enumerate(y1):
    y[i,0] = value
fig = plt.figure(figsize=(10,10))
plt.plot(X, y, 'b.')
plt.show()

df = pd.read_csv('predicting.csv', header=None)
X_2 = df[0].to_numpy()
X2 = np.zeros(shape=(X_2.shape[0],1))
for i, value in enumerate(X_2):
    X2[i,0] = value
y_2 = df[1].to_numpy()
y2 = np.zeros(shape=(y_2.shape[0],1))
for i, value in enumerate(y_2):
    y2[i,0] = value
plt.plot(X2, y2, 'g.')
plt.show()

alpha = 0.0006  # learning rate
n_iterations = 1000

M = X.shape[0]
# add x0 = 1 to each instance.
X_b = np.c_[np.ones((M, 1)), X]
Joptimum, a_opt = calculateClosedFormSolution(X_b, y, M)
print('Closed-Form Solution\na0: ' + str(a_opt[0, 0]) + '\na1: ' + str(a_opt[1, 0]) + '\n')

# Run batch gradient-descent algorithm.
a, a_hist, Jgd = batchGradientDescent(alpha, n_iterations, X_b, y)
print('Batch Gradient-Descent Algorithm (alpha=' + str(alpha) + ')\na0: ' + str(a_hist[0, n_iterations]) + '\na1: ' + str(a_hist[1, n_iterations]) + '\n')
plotErrorVsIteration(Jgd, n_iterations)
h = hypothesis(X_b, a_hist[0, n_iterations], a_hist[1, n_iterations])
plt.title('x Vs h (training)')
plt.plot(X, h, 'r.')
plt.show()
plt.title('x Vs h (predicting)')
plt.plot(X2, h, 'r.')
plt.show()

X_b = np.c_[np.ones((M, 1)), X_b]
Joptimum, a_opt = calculateClosedFormSolution(X_b, y, M)
print('Closed-Form Solution\na0: ' + str(a_opt[0, 0]) + '\na1: ' + str(a_opt[1, 0]) + '\na2: ' + str(a_opt[2, 0]) + '\n')

# Run batch gradient-descent algorithm.
a, a_hist, Jgd = batchGradientDescent(alpha, n_iterations, X_b, y)
print('Batch Gradient-Descent Algorithm (alpha=' + str(alpha) + ')\na0: ' + str(a_hist[0, n_iterations]) + '\na1: ' + str(a_hist[1, n_iterations]) + '\na2: ' + str(a_hist[2, n_iterations]) + '\n')
plotErrorVsIteration(Jgd, n_iterations)
h = hypothesis(X_b, a_hist[0, n_iterations], a_hist[1, n_iterations], a_hist[2, n_iterations])
plt.title('x Vs h (training)')
plt.plot(X, h, 'r.')
plt.show()
plt.title('x Vs h (predicting)')
plt.plot(X2, h, 'r.')
plt.show()

X_b = np.c_[np.ones((M, 1)), X_b]
Joptimum, a_opt = calculateClosedFormSolution(X_b, y, M)
print('Closed-Form Solution\na0: ' + str(a_opt[0, 0]) + '\na1: ' + str(a_opt[1, 0]) + '\na2: ' + str(a_opt[2, 0]) + '\na3: ' + str(a_opt[3, 0]) + '\n' )

# Run batch gradient-descent algorithm.
a, a_hist, Jgd = batchGradientDescent(alpha, n_iterations, X_b, y)
print('Batch Gradient-Descent Algorithm (alpha=' + str(alpha) + ')\na0: ' + str(a_hist[0, n_iterations]) + '\na1: ' + str(a_hist[1, n_iterations]) + '\na2: ' + str(a_hist[2, n_iterations]) + '\na3: ' + str(a_hist[3, n_iterations]) + '\n')
plotErrorVsIteration(Jgd, n_iterations)
h = hypothesis(X_b, a_hist[0, n_iterations], a_hist[1, n_iterations], a_hist[2, n_iterations], a_hist[3, n_iterations])
plt.title('x Vs h (training)')
plt.plot(X, h, 'r.')
plt.show()
plt.title('x Vs h (predicting)')
plt.plot(X2, h, 'r.')
plt.show()

X_b = np.c_[np.ones((M, 1)), X_b]
Joptimum, a_opt = calculateClosedFormSolution(X_b, y, M)
print('Closed-Form Solution\na0: ' + str(a_opt[0, 0]) + '\na1: ' + str(a_opt[1, 0]) + '\na2: ' + str(a_opt[2, 0]) + '\na3: ' + str(a_opt[3, 0]) + '\na4: ' + str(a_opt[4, 0]) + '\n')

# Run batch gradient-descent algorithm.
a, a_hist, Jgd = batchGradientDescent(alpha, n_iterations, X_b, y)
print('Batch Gradient-Descent Algorithm (alpha=' + str(alpha) + ')\na0: ' + str(a_hist[0, n_iterations]) + '\na1: ' + str(a_hist[1, n_iterations]) + '\na2: ' + str(a_hist[2, n_iterations]) + '\na3: ' + str(a_hist[2, n_iterations]) + '\na4: ' + str(a_hist[2, n_iterations]) + '\n')
plotErrorVsIteration(Jgd, n_iterations)
h = hypothesis(X_b, a_hist[0, n_iterations], a_hist[1, n_iterations], a_hist[2, n_iterations], a_hist[3, n_iterations], a_hist[4, n_iterations])
plt.title('x Vs h (training)')
plt.plot(X, h, 'r.')
plt.show()
plt.title('x Vs h (predicting)')
plt.plot(X2, h, 'r.')
plt.show()

# c) O valor ótimo do passo de aprendizagem é 0.0006
# A) A função hipótese que aproxima melhor a função alvo é a d (h = a0 + a1*x + a2*x^2 + a3*x^3 + a4*x^4)
