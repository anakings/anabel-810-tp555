{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import tfgraphviz as tfg\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 2\n",
    "n_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        if activation==\"sigmoid\":\n",
    "            return tf.math.sigmoid(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Number of examples.\n",
    "N = 1000\n",
    "\n",
    "x1 = np.random.randint(0,2,N)\n",
    "x2 = np.random.randint(0,2,N)\n",
    "\n",
    "y_xor = x1 ^ x2\n",
    "\n",
    "x1 = x1 + 0.1*np.random.randn(N,)\n",
    "x2 = x2 + 0.1*np.random.randn(N,)\n",
    "\n",
    "x_xor = np.zeros((len(x1), 2))\n",
    "for i in range(len(x1)):\n",
    "    x_xor[i,0] = x1[i]\n",
    "    x_xor[i,1] = x2[i]\n",
    "\n",
    "# Split array into random train and test subsets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_xor, y_xor, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"sigmoid\")\n",
    "    logits = neuron_layer(hidden1, n_outputs, \"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b)\n",
      "0 Train loss: 0.75627196 Test loss: 0.780114\n",
      "100 Train loss: 0.69866675 Test loss: 0.6983298\n",
      "200 Train loss: 0.68950045 Test loss: 0.6914551\n",
      "300 Train loss: 0.63579303 Test loss: 0.65702575\n",
      "400 Train loss: 0.4905203 Test loss: 0.51534486\n",
      "500 Train loss: 0.15395682 Test loss: 0.16490595\n",
      "600 Train loss: 0.07326271 Test loss: 0.07252537\n",
      "700 Train loss: 0.040938858 Test loss: 0.044510074\n",
      "800 Train loss: 0.03016437 Test loss: 0.031788163\n",
      "900 Train loss: 0.026076388 Test loss: 0.02465648\n",
      "1000 Train loss: 0.017030884 Test loss: 0.020129258\n",
      "1100 Train loss: 0.016878147 Test loss: 0.017012963\n",
      "1200 Train loss: 0.011982724 Test loss: 0.014739778\n",
      "1300 Train loss: 0.01001075 Test loss: 0.013012176\n",
      "1400 Train loss: 0.010740886 Test loss: 0.011653997\n",
      "1500 Train loss: 0.007985737 Test loss: 0.01055999\n",
      "1600 Train loss: 0.008321167 Test loss: 0.009658799\n",
      "1700 Train loss: 0.007176362 Test loss: 0.008904287\n",
      "1800 Train loss: 0.006182943 Test loss: 0.008262905\n",
      "1900 Train loss: 0.006172646 Test loss: 0.0077110045\n",
      "2000 Train loss: 0.0052496176 Test loss: 0.0072313542\n",
      "2100 Train loss: 0.005847431 Test loss: 0.0068103247\n",
      "2200 Train loss: 0.0054045366 Test loss: 0.00643707\n",
      "2300 Train loss: 0.0051798415 Test loss: 0.0061049443\n",
      "2400 Train loss: 0.0070967195 Test loss: 0.005807012\n",
      "2500 Train loss: 0.004562754 Test loss: 0.005537959\n",
      "2600 Train loss: 0.0039132377 Test loss: 0.005293828\n",
      "2700 Train loss: 0.0072547346 Test loss: 0.0050717746\n",
      "2800 Train loss: 0.005163659 Test loss: 0.0048677656\n",
      "2900 Train loss: 0.0034229027 Test loss: 0.0046813968\n",
      "3000 Train loss: 0.0033134425 Test loss: 0.0045086457\n",
      "3100 Train loss: 0.0029559953 Test loss: 0.0043486278\n",
      "3200 Train loss: 0.003165132 Test loss: 0.004200697\n",
      "3300 Train loss: 0.0038906734 Test loss: 0.0040625967\n",
      "3400 Train loss: 0.0028360938 Test loss: 0.0039339135\n",
      "3500 Train loss: 0.008142185 Test loss: 0.0038138886\n",
      "3600 Train loss: 0.0035110181 Test loss: 0.0037008005\n",
      "3700 Train loss: 0.00457572 Test loss: 0.0035951897\n",
      "3800 Train loss: 0.0023357954 Test loss: 0.0034949833\n",
      "3900 Train loss: 0.007493187 Test loss: 0.0034011593\n",
      "4000 Train loss: 0.002294282 Test loss: 0.0033119463\n",
      "4100 Train loss: 0.005947392 Test loss: 0.0032279338\n",
      "4200 Train loss: 0.005491842 Test loss: 0.0031478575\n",
      "4300 Train loss: 0.0034643624 Test loss: 0.0030721421\n",
      "4400 Train loss: 0.0018164869 Test loss: 0.0030001465\n",
      "4500 Train loss: 0.0038044045 Test loss: 0.0029311876\n",
      "4600 Train loss: 0.0016489534 Test loss: 0.0028660013\n",
      "4700 Train loss: 0.0016313287 Test loss: 0.0028035098\n",
      "4800 Train loss: 0.003300095 Test loss: 0.0027440826\n",
      "4900 Train loss: 0.002822554 Test loss: 0.002687113\n",
      "5000 Train loss: 0.002587584 Test loss: 0.0026322617\n",
      "5100 Train loss: 0.0016953875 Test loss: 0.0025799805\n",
      "5200 Train loss: 0.0016333119 Test loss: 0.0025299184\n",
      "5300 Train loss: 0.0033675483 Test loss: 0.0024815754\n",
      "5400 Train loss: 0.0014633253 Test loss: 0.0024353706\n",
      "5500 Train loss: 0.001376321 Test loss: 0.002390674\n",
      "5600 Train loss: 0.0014287676 Test loss: 0.002347859\n",
      "5700 Train loss: 0.0015642481 Test loss: 0.0023065694\n",
      "5800 Train loss: 0.0012941323 Test loss: 0.0022666608\n",
      "5900 Train loss: 0.004701892 Test loss: 0.0022283406\n",
      "6000 Train loss: 0.00415804 Test loss: 0.0021909764\n",
      "6100 Train loss: 0.002525537 Test loss: 0.0021552367\n",
      "6200 Train loss: 0.0027981573 Test loss: 0.0021205265\n",
      "6300 Train loss: 0.0019215795 Test loss: 0.0020872192\n",
      "6400 Train loss: 0.0011248806 Test loss: 0.0020547218\n",
      "6500 Train loss: 0.0012490565 Test loss: 0.002023296\n",
      "6600 Train loss: 0.0012874425 Test loss: 0.0019928904\n",
      "6700 Train loss: 0.0010286764 Test loss: 0.0019632587\n",
      "6800 Train loss: 0.003909014 Test loss: 0.001934692\n",
      "6900 Train loss: 0.0011302461 Test loss: 0.0019072017\n",
      "7000 Train loss: 0.0026658215 Test loss: 0.001880291\n",
      "7100 Train loss: 0.0011035354 Test loss: 0.0018540513\n",
      "7200 Train loss: 0.0010028144 Test loss: 0.0018286003\n",
      "7300 Train loss: 0.0010725583 Test loss: 0.0018039665\n",
      "7400 Train loss: 0.0010364318 Test loss: 0.0017798308\n",
      "7500 Train loss: 0.00096169865 Test loss: 0.0017561212\n",
      "7600 Train loss: 0.0009991587 Test loss: 0.0017335925\n",
      "7700 Train loss: 0.00093360874 Test loss: 0.001711565\n",
      "7800 Train loss: 0.002597545 Test loss: 0.0016900317\n",
      "7900 Train loss: 0.0022110566 Test loss: 0.0016690755\n",
      "8000 Train loss: 0.0022341036 Test loss: 0.0016483858\n",
      "8100 Train loss: 0.0037817734 Test loss: 0.001628263\n",
      "8200 Train loss: 0.0010156236 Test loss: 0.0016088347\n",
      "8300 Train loss: 0.0010554667 Test loss: 0.0015896248\n",
      "8400 Train loss: 0.0020445539 Test loss: 0.001571259\n",
      "8500 Train loss: 0.001427806 Test loss: 0.0015531275\n",
      "8600 Train loss: 0.0023938897 Test loss: 0.0015353793\n",
      "8700 Train loss: 0.0014566836 Test loss: 0.0015181116\n",
      "8800 Train loss: 0.002385843 Test loss: 0.0015012748\n",
      "8900 Train loss: 0.00088840217 Test loss: 0.0014848122\n",
      "9000 Train loss: 0.00081869843 Test loss: 0.0014685101\n",
      "9100 Train loss: 0.0007900573 Test loss: 0.0014526392\n",
      "9200 Train loss: 0.00085652125 Test loss: 0.0014374147\n",
      "9300 Train loss: 0.0049265753 Test loss: 0.0014222316\n",
      "9400 Train loss: 0.0038821641 Test loss: 0.0014074943\n",
      "9500 Train loss: 0.004876586 Test loss: 0.001393025\n",
      "9600 Train loss: 0.0018387479 Test loss: 0.0013786469\n",
      "9700 Train loss: 0.0007231155 Test loss: 0.0013648255\n",
      "9800 Train loss: 0.00064618787 Test loss: 0.0013511913\n",
      "9900 Train loss: 0.00066997355 Test loss: 0.0013378542\n",
      "10000 Train loss: 0.000767564 Test loss: 0.0013247081\n",
      "10100 Train loss: 0.003344642 Test loss: 0.001311943\n",
      "10200 Train loss: 0.00062369916 Test loss: 0.001299318\n",
      "10300 Train loss: 0.0031790265 Test loss: 0.0012870516\n",
      "10400 Train loss: 0.002169113 Test loss: 0.0012750205\n",
      "10500 Train loss: 0.0006216879 Test loss: 0.0012631352\n",
      "10600 Train loss: 0.0021165735 Test loss: 0.0012514686\n",
      "10700 Train loss: 0.00066164567 Test loss: 0.0012400178\n",
      "10800 Train loss: 0.0030506079 Test loss: 0.001228798\n",
      "10900 Train loss: 0.00061208796 Test loss: 0.0012178838\n",
      "11000 Train loss: 0.00054055103 Test loss: 0.0012068776\n",
      "11100 Train loss: 0.0045001144 Test loss: 0.0011962752\n",
      "11200 Train loss: 0.0006392289 Test loss: 0.0011859772\n",
      "11300 Train loss: 0.0029950724 Test loss: 0.0011756597\n",
      "11400 Train loss: 0.0030214756 Test loss: 0.0011655177\n",
      "11500 Train loss: 0.0020334774 Test loss: 0.0011556749\n",
      "11600 Train loss: 0.002944311 Test loss: 0.0011458995\n",
      "11700 Train loss: 0.0009488562 Test loss: 0.0011363889\n",
      "11800 Train loss: 0.00059920363 Test loss: 0.001126954\n",
      "11900 Train loss: 0.0005387949 Test loss: 0.0011177314\n",
      "12000 Train loss: 0.0019113804 Test loss: 0.0011085751\n",
      "12100 Train loss: 0.0016046657 Test loss: 0.0010995887\n",
      "12200 Train loss: 0.0005470512 Test loss: 0.0010908364\n",
      "12300 Train loss: 0.0021686894 Test loss: 0.0010820819\n",
      "12400 Train loss: 0.0008896036 Test loss: 0.0010735055\n",
      "12500 Train loss: 0.00051185786 Test loss: 0.0010652117\n",
      "12600 Train loss: 0.000456849 Test loss: 0.0010569631\n",
      "12700 Train loss: 0.0028516673 Test loss: 0.0010487551\n",
      "12800 Train loss: 0.0005523985 Test loss: 0.0010407924\n",
      "12900 Train loss: 0.0017843063 Test loss: 0.0010329203\n",
      "13000 Train loss: 0.002135112 Test loss: 0.0010250951\n",
      "13100 Train loss: 0.00046544187 Test loss: 0.0010174105\n",
      "13200 Train loss: 0.00044174652 Test loss: 0.0010097539\n",
      "13300 Train loss: 0.00079005345 Test loss: 0.0010023416\n",
      "13400 Train loss: 0.0005109682 Test loss: 0.0009950849\n",
      "13500 Train loss: 0.00049523474 Test loss: 0.000987927\n",
      "13600 Train loss: 0.0017991014 Test loss: 0.0009808037\n",
      "13700 Train loss: 0.00083649444 Test loss: 0.0009738182\n",
      "13800 Train loss: 0.0015514867 Test loss: 0.0009668921\n",
      "13900 Train loss: 0.0016251354 Test loss: 0.000960059\n",
      "14000 Train loss: 0.0005112415 Test loss: 0.00095337996\n",
      "14100 Train loss: 0.00044787486 Test loss: 0.0009466657\n",
      "14200 Train loss: 0.0004937446 Test loss: 0.00094024366\n",
      "14300 Train loss: 0.0026752173 Test loss: 0.00093367894\n",
      "14400 Train loss: 0.0007211896 Test loss: 0.0009274583\n",
      "14500 Train loss: 0.00039753818 Test loss: 0.00092110777\n",
      "14600 Train loss: 0.00048229832 Test loss: 0.00091488747\n",
      "14700 Train loss: 0.0027472049 Test loss: 0.0009087831\n",
      "14800 Train loss: 0.00152465 Test loss: 0.000902754\n",
      "14900 Train loss: 0.0004849438 Test loss: 0.00089692656\n",
      "15000 Train loss: 0.0016802506 Test loss: 0.00089098903\n",
      "15100 Train loss: 0.0004490399 Test loss: 0.0008853788\n",
      "15200 Train loss: 0.0025207032 Test loss: 0.00087956764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15300 Train loss: 0.00047845612 Test loss: 0.0008739225\n",
      "15400 Train loss: 0.000494819 Test loss: 0.0008682547\n",
      "15500 Train loss: 0.00243787 Test loss: 0.0008628015\n",
      "15600 Train loss: 0.0004597604 Test loss: 0.00085737294\n",
      "15700 Train loss: 0.0017591239 Test loss: 0.00085207756\n",
      "15800 Train loss: 0.0003903003 Test loss: 0.0008467627\n",
      "15900 Train loss: 0.00045938537 Test loss: 0.0008415652\n",
      "16000 Train loss: 0.00042798772 Test loss: 0.00083625905\n",
      "16100 Train loss: 0.0014442847 Test loss: 0.0008312368\n",
      "16200 Train loss: 0.0024367974 Test loss: 0.00082622207\n",
      "16300 Train loss: 0.00047526913 Test loss: 0.0008212119\n",
      "16400 Train loss: 0.0016206136 Test loss: 0.00081629393\n",
      "16500 Train loss: 0.0003248232 Test loss: 0.00081144046\n",
      "16600 Train loss: 0.0004405087 Test loss: 0.00080662087\n",
      "16700 Train loss: 0.0024741855 Test loss: 0.00080175424\n",
      "16800 Train loss: 0.0013906942 Test loss: 0.00079715595\n",
      "16900 Train loss: 0.00038194365 Test loss: 0.0007925512\n",
      "17000 Train loss: 0.00041891017 Test loss: 0.0007879481\n",
      "17100 Train loss: 0.00038407106 Test loss: 0.00078340265\n",
      "17200 Train loss: 0.00035741087 Test loss: 0.0007787946\n",
      "17300 Train loss: 0.0005495831 Test loss: 0.0007743564\n",
      "17400 Train loss: 0.00034622493 Test loss: 0.00076994667\n",
      "17500 Train loss: 0.0015611828 Test loss: 0.00076572056\n",
      "17600 Train loss: 0.0025444087 Test loss: 0.00076148385\n",
      "17700 Train loss: 0.0023744018 Test loss: 0.00075726735\n",
      "17800 Train loss: 0.00037683907 Test loss: 0.00075295864\n",
      "17900 Train loss: 0.002284017 Test loss: 0.00074883556\n",
      "18000 Train loss: 0.00031877455 Test loss: 0.0007446726\n",
      "18100 Train loss: 0.00031071916 Test loss: 0.0007406995\n",
      "18200 Train loss: 0.0013236561 Test loss: 0.00073676417\n",
      "18300 Train loss: 0.00046944746 Test loss: 0.0007327165\n",
      "18400 Train loss: 0.0002850966 Test loss: 0.000728761\n",
      "18500 Train loss: 0.00042571308 Test loss: 0.00072491367\n",
      "18600 Train loss: 0.001295669 Test loss: 0.0007210139\n",
      "18700 Train loss: 0.0022477037 Test loss: 0.0007171913\n",
      "18800 Train loss: 0.00032711652 Test loss: 0.0007134656\n",
      "18900 Train loss: 0.0015184773 Test loss: 0.0007097184\n",
      "19000 Train loss: 0.00024878778 Test loss: 0.0007060408\n",
      "19100 Train loss: 0.00033053258 Test loss: 0.00070232677\n",
      "19200 Train loss: 0.00054922147 Test loss: 0.0006987327\n",
      "19300 Train loss: 0.0003680528 Test loss: 0.0006951349\n",
      "19400 Train loss: 0.0004096427 Test loss: 0.0006915847\n",
      "19500 Train loss: 0.0003188469 Test loss: 0.0006881567\n",
      "19600 Train loss: 0.0003410581 Test loss: 0.000684595\n",
      "19700 Train loss: 0.0032880846 Test loss: 0.0006811945\n",
      "19800 Train loss: 0.001195704 Test loss: 0.00067766645\n",
      "19900 Train loss: 0.00030522756 Test loss: 0.0006744758\n",
      "20000 Train loss: 0.0002688608 Test loss: 0.00067108206\n",
      "20100 Train loss: 0.0003047126 Test loss: 0.0006678345\n",
      "20200 Train loss: 0.0012321856 Test loss: 0.00066452747\n",
      "20300 Train loss: 0.00025789445 Test loss: 0.00066130824\n",
      "20400 Train loss: 0.0003376241 Test loss: 0.00065808714\n",
      "20500 Train loss: 0.0031798326 Test loss: 0.0006549435\n",
      "20600 Train loss: 0.00025510465 Test loss: 0.00065174844\n",
      "20700 Train loss: 0.000335568 Test loss: 0.0006486332\n",
      "20800 Train loss: 0.0003106507 Test loss: 0.00064558303\n",
      "20900 Train loss: 0.00029586488 Test loss: 0.0006424861\n",
      "21000 Train loss: 0.0015249274 Test loss: 0.0006394785\n",
      "21100 Train loss: 0.0011924199 Test loss: 0.0006364081\n",
      "21200 Train loss: 0.002149444 Test loss: 0.000633417\n",
      "21300 Train loss: 0.00213702 Test loss: 0.0006305226\n",
      "21400 Train loss: 0.0013288482 Test loss: 0.0006275674\n",
      "21500 Train loss: 0.0012344612 Test loss: 0.00062463636\n",
      "21600 Train loss: 0.0012323419 Test loss: 0.0006218259\n",
      "21700 Train loss: 0.0002748656 Test loss: 0.0006189109\n",
      "21800 Train loss: 0.0002805484 Test loss: 0.00061609905\n",
      "21900 Train loss: 0.00043139394 Test loss: 0.00061333884\n",
      "22000 Train loss: 0.00022195802 Test loss: 0.000610547\n",
      "22100 Train loss: 0.00026553092 Test loss: 0.0006077887\n",
      "22200 Train loss: 0.0003479492 Test loss: 0.00060514314\n",
      "22300 Train loss: 0.00023569498 Test loss: 0.00060237414\n",
      "22400 Train loss: 0.0002592847 Test loss: 0.0005997112\n",
      "22500 Train loss: 0.0030438728 Test loss: 0.0005970893\n",
      "22600 Train loss: 0.0020280315 Test loss: 0.000594444\n",
      "22700 Train loss: 0.002224657 Test loss: 0.00059180136\n",
      "22800 Train loss: 0.0002854843 Test loss: 0.00058926875\n",
      "22900 Train loss: 0.00025293857 Test loss: 0.00058670173\n",
      "23000 Train loss: 0.0021900034 Test loss: 0.00058414135\n",
      "23100 Train loss: 0.0014482536 Test loss: 0.0005815729\n",
      "23200 Train loss: 0.00024565123 Test loss: 0.00057913\n",
      "23300 Train loss: 0.00022154987 Test loss: 0.0005766726\n",
      "23400 Train loss: 0.00046687326 Test loss: 0.0005741925\n",
      "23500 Train loss: 0.00043729655 Test loss: 0.000571779\n",
      "23600 Train loss: 0.0002255971 Test loss: 0.0005693574\n",
      "23700 Train loss: 0.00024042418 Test loss: 0.0005669364\n",
      "23800 Train loss: 0.0002537417 Test loss: 0.0005646135\n",
      "23900 Train loss: 0.0012586456 Test loss: 0.0005622732\n",
      "24000 Train loss: 0.00030972424 Test loss: 0.0005599188\n",
      "24100 Train loss: 0.00040612288 Test loss: 0.00055759936\n",
      "24200 Train loss: 0.00027070378 Test loss: 0.0005553068\n",
      "24300 Train loss: 0.0004271058 Test loss: 0.00055305916\n",
      "24400 Train loss: 0.00026709802 Test loss: 0.00055078504\n",
      "24500 Train loss: 0.00022442485 Test loss: 0.0005485104\n",
      "24600 Train loss: 0.00033423785 Test loss: 0.0005462712\n",
      "24700 Train loss: 0.0019095009 Test loss: 0.0005441686\n",
      "24800 Train loss: 0.00197003 Test loss: 0.0005419692\n",
      "24900 Train loss: 0.00022256727 Test loss: 0.0005397743\n",
      "25000 Train loss: 0.0019057861 Test loss: 0.00053760567\n",
      "25100 Train loss: 0.00026777567 Test loss: 0.0005354737\n",
      "25200 Train loss: 0.0002863983 Test loss: 0.000533338\n",
      "25300 Train loss: 0.00019828852 Test loss: 0.00053120474\n",
      "25400 Train loss: 0.00026698067 Test loss: 0.0005291196\n",
      "25500 Train loss: 0.0022081202 Test loss: 0.0005270661\n",
      "25600 Train loss: 0.00025064137 Test loss: 0.000524979\n",
      "25700 Train loss: 0.0012027989 Test loss: 0.0005229161\n",
      "25800 Train loss: 0.00039195738 Test loss: 0.00052090327\n",
      "25900 Train loss: 0.00038075418 Test loss: 0.00051892357\n",
      "26000 Train loss: 0.00019847405 Test loss: 0.0005169277\n",
      "26100 Train loss: 0.00024208265 Test loss: 0.00051492586\n",
      "26200 Train loss: 0.00023075569 Test loss: 0.0005129753\n",
      "26300 Train loss: 0.00026329744 Test loss: 0.00051097677\n",
      "26400 Train loss: 0.00041258513 Test loss: 0.0005090051\n",
      "26500 Train loss: 0.001100166 Test loss: 0.00050709856\n",
      "26600 Train loss: 0.001162339 Test loss: 0.0005051955\n",
      "26700 Train loss: 0.00022636034 Test loss: 0.000503385\n",
      "26800 Train loss: 0.00020978937 Test loss: 0.000501519\n",
      "26900 Train loss: 0.00020561888 Test loss: 0.00049957476\n",
      "27000 Train loss: 0.00021948213 Test loss: 0.0004977478\n",
      "27100 Train loss: 0.0002110792 Test loss: 0.0004958366\n",
      "27200 Train loss: 0.00024856091 Test loss: 0.0004940525\n",
      "27300 Train loss: 0.0010521659 Test loss: 0.0004922434\n",
      "27400 Train loss: 0.00023999458 Test loss: 0.0004904372\n",
      "27500 Train loss: 0.0001859607 Test loss: 0.00048859406\n",
      "27600 Train loss: 0.0002003918 Test loss: 0.00048683127\n",
      "27700 Train loss: 0.0026008114 Test loss: 0.00048510107\n",
      "27800 Train loss: 0.00036746997 Test loss: 0.00048337007\n",
      "27900 Train loss: 0.00034752555 Test loss: 0.00048163373\n",
      "28000 Train loss: 0.0012804616 Test loss: 0.0004799428\n",
      "28100 Train loss: 0.00018358225 Test loss: 0.00047822067\n",
      "28200 Train loss: 0.00018098844 Test loss: 0.00047649813\n",
      "28300 Train loss: 0.00023651132 Test loss: 0.00047475094\n",
      "28400 Train loss: 0.00024138365 Test loss: 0.0004731215\n",
      "28500 Train loss: 0.0010098714 Test loss: 0.00047140833\n",
      "28600 Train loss: 0.00017529548 Test loss: 0.00046976894\n",
      "28700 Train loss: 0.00023966697 Test loss: 0.00046813145\n",
      "28800 Train loss: 0.00018700083 Test loss: 0.00046653472\n",
      "28900 Train loss: 0.00022937296 Test loss: 0.00046493075\n",
      "29000 Train loss: 0.0001908 Test loss: 0.00046331462\n",
      "29100 Train loss: 0.0001984729 Test loss: 0.0004617589\n",
      "29200 Train loss: 0.00020785825 Test loss: 0.0004601209\n",
      "29300 Train loss: 0.0011688732 Test loss: 0.00045852512\n",
      "29400 Train loss: 0.0019322251 Test loss: 0.00045690892\n",
      "29500 Train loss: 0.00019585481 Test loss: 0.00045535085\n",
      "29600 Train loss: 0.00020018683 Test loss: 0.00045384536\n",
      "29700 Train loss: 0.0009689693 Test loss: 0.0004522835\n",
      "29800 Train loss: 0.0001869715 Test loss: 0.0004507703\n",
      "29900 Train loss: 0.0017183666 Test loss: 0.00044922368\n",
      "30000 Train loss: 0.00022294503 Test loss: 0.0004477597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30100 Train loss: 0.0001651643 Test loss: 0.00044625095\n",
      "30200 Train loss: 0.0025745074 Test loss: 0.00044471788\n",
      "30300 Train loss: 0.0024785325 Test loss: 0.00044324537\n",
      "30400 Train loss: 0.000200393 Test loss: 0.00044179373\n",
      "30500 Train loss: 0.0001745637 Test loss: 0.00044036054\n",
      "30600 Train loss: 0.00018463875 Test loss: 0.00043889845\n",
      "30700 Train loss: 0.00029386865 Test loss: 0.00043745583\n",
      "30800 Train loss: 0.00030791454 Test loss: 0.000436016\n",
      "30900 Train loss: 0.00017067243 Test loss: 0.00043458518\n",
      "31000 Train loss: 0.00020650365 Test loss: 0.00043316005\n",
      "31100 Train loss: 0.0017919163 Test loss: 0.00043178213\n",
      "31200 Train loss: 0.0001415117 Test loss: 0.0004303797\n",
      "31300 Train loss: 0.00018432502 Test loss: 0.0004290236\n",
      "31400 Train loss: 0.00018505716 Test loss: 0.00042763766\n",
      "31500 Train loss: 0.0016861028 Test loss: 0.0004262726\n",
      "31600 Train loss: 0.00013564985 Test loss: 0.00042489188\n",
      "31700 Train loss: 0.0010492189 Test loss: 0.00042353768\n",
      "31800 Train loss: 0.00018741332 Test loss: 0.00042216748\n",
      "31900 Train loss: 0.00015501906 Test loss: 0.00042085486\n",
      "32000 Train loss: 0.00014877529 Test loss: 0.0004195025\n",
      "32100 Train loss: 0.00018122136 Test loss: 0.00041821404\n",
      "32200 Train loss: 0.00019050646 Test loss: 0.00041694503\n",
      "32300 Train loss: 0.0016616588 Test loss: 0.00041564857\n",
      "32400 Train loss: 0.00018248502 Test loss: 0.00041429434\n",
      "32500 Train loss: 0.00019678546 Test loss: 0.00041303565\n",
      "32600 Train loss: 0.00016263849 Test loss: 0.0004117397\n",
      "32700 Train loss: 0.00014509262 Test loss: 0.0004104834\n",
      "32800 Train loss: 0.0001970221 Test loss: 0.0004091713\n",
      "32900 Train loss: 0.00014164859 Test loss: 0.00040795383\n",
      "33000 Train loss: 0.0001307087 Test loss: 0.00040666724\n",
      "33100 Train loss: 0.0001877144 Test loss: 0.0004054417\n",
      "33200 Train loss: 0.0001824837 Test loss: 0.00040421667\n",
      "33300 Train loss: 0.0008852176 Test loss: 0.00040301622\n",
      "33400 Train loss: 0.00015349821 Test loss: 0.00040180486\n",
      "33500 Train loss: 0.0001490646 Test loss: 0.00040055232\n",
      "33600 Train loss: 0.0023307933 Test loss: 0.00039934387\n",
      "33700 Train loss: 0.0001804621 Test loss: 0.00039815623\n",
      "33800 Train loss: 0.003239486 Test loss: 0.0003969822\n",
      "33900 Train loss: 0.00012848206 Test loss: 0.00039582295\n",
      "34000 Train loss: 0.00030315242 Test loss: 0.00039463094\n",
      "34100 Train loss: 0.00014968663 Test loss: 0.0003934376\n",
      "34200 Train loss: 0.0010774088 Test loss: 0.00039227694\n",
      "34300 Train loss: 0.00017760904 Test loss: 0.00039114416\n",
      "34400 Train loss: 0.00015013744 Test loss: 0.0003899697\n",
      "34500 Train loss: 0.00012850044 Test loss: 0.00038882744\n",
      "34600 Train loss: 0.0009792605 Test loss: 0.0003877136\n",
      "34700 Train loss: 0.00014285647 Test loss: 0.0003865936\n",
      "34800 Train loss: 0.00019129647 Test loss: 0.0003854575\n",
      "34900 Train loss: 0.0030679165 Test loss: 0.00038430956\n",
      "35000 Train loss: 0.00014746297 Test loss: 0.00038319523\n",
      "35100 Train loss: 0.00017486226 Test loss: 0.00038208935\n",
      "35200 Train loss: 0.00095452124 Test loss: 0.0003810025\n",
      "35300 Train loss: 0.0010757513 Test loss: 0.00037988866\n",
      "35400 Train loss: 0.00023109547 Test loss: 0.00037883729\n",
      "35500 Train loss: 0.00012736268 Test loss: 0.00037778163\n",
      "35600 Train loss: 0.0001505391 Test loss: 0.00037664358\n",
      "35700 Train loss: 0.00014073234 Test loss: 0.0003756282\n",
      "35800 Train loss: 0.00012906341 Test loss: 0.0003745768\n",
      "35900 Train loss: 0.00014542566 Test loss: 0.00037352828\n",
      "36000 Train loss: 0.0001301075 Test loss: 0.0003724645\n",
      "36100 Train loss: 0.0010522753 Test loss: 0.00037137428\n",
      "36200 Train loss: 0.0023221844 Test loss: 0.00037034412\n",
      "36300 Train loss: 0.00014335793 Test loss: 0.00036933774\n",
      "36400 Train loss: 0.00016720698 Test loss: 0.00036828205\n",
      "36500 Train loss: 0.00014400782 Test loss: 0.00036726327\n",
      "36600 Train loss: 0.0014974838 Test loss: 0.00036624743\n",
      "36700 Train loss: 0.00014802034 Test loss: 0.00036524385\n",
      "36800 Train loss: 0.00014891247 Test loss: 0.00036422795\n",
      "36900 Train loss: 0.00011538023 Test loss: 0.00036323906\n",
      "37000 Train loss: 0.00016716812 Test loss: 0.0003622041\n",
      "37100 Train loss: 0.000132987 Test loss: 0.00036123992\n",
      "37200 Train loss: 0.0015238231 Test loss: 0.00036025423\n",
      "37300 Train loss: 0.0021766499 Test loss: 0.0003593094\n",
      "37400 Train loss: 0.00024495093 Test loss: 0.00035829918\n",
      "37500 Train loss: 0.0001244068 Test loss: 0.00035731966\n",
      "37600 Train loss: 0.0009288841 Test loss: 0.00035637047\n",
      "37700 Train loss: 0.00092861126 Test loss: 0.00035537634\n",
      "37800 Train loss: 0.000114482464 Test loss: 0.00035444426\n",
      "37900 Train loss: 0.00085501897 Test loss: 0.00035346946\n",
      "38000 Train loss: 0.00014366585 Test loss: 0.00035253924\n",
      "38100 Train loss: 0.00012987344 Test loss: 0.00035158862\n",
      "38200 Train loss: 0.00011214751 Test loss: 0.0003506276\n",
      "38300 Train loss: 0.00021855913 Test loss: 0.00034970968\n",
      "38400 Train loss: 0.0002197327 Test loss: 0.0003488387\n",
      "38500 Train loss: 0.0014859273 Test loss: 0.0003478957\n",
      "38600 Train loss: 0.0022159067 Test loss: 0.00034699618\n",
      "38700 Train loss: 0.00012410202 Test loss: 0.00034607775\n",
      "38800 Train loss: 0.00011942989 Test loss: 0.00034517786\n",
      "38900 Train loss: 0.0007958188 Test loss: 0.000344259\n",
      "39000 Train loss: 0.00014237312 Test loss: 0.00034340224\n",
      "39100 Train loss: 0.00013703686 Test loss: 0.0003425028\n",
      "39200 Train loss: 0.000119840915 Test loss: 0.0003416256\n",
      "39300 Train loss: 0.0021134939 Test loss: 0.0003407161\n",
      "39400 Train loss: 0.00013416666 Test loss: 0.00033982706\n",
      "39500 Train loss: 0.00081448356 Test loss: 0.00033893468\n",
      "39600 Train loss: 0.000120614175 Test loss: 0.00033810345\n",
      "39700 Train loss: 0.00012694341 Test loss: 0.00033723484\n",
      "39800 Train loss: 0.00015393255 Test loss: 0.00033637043\n",
      "39900 Train loss: 0.00010787965 Test loss: 0.00033549604\n"
     ]
    }
   ],
   "source": [
    "print('b)')\n",
    "n_epochs = 40000\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_test = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, \"Train loss:\", loss_train, \"Test loss:\", loss_test)\n",
    "    save_path = saver.save(sess, \"./my_model_finalv1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_finalv1.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_finalv1.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: X_valid})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientDescentOptimizer accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print('GradientDescentOptimizer accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"sigmoid\")\n",
    "    logits = neuron_layer(hidden1, n_outputs, \"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d)\n",
      "0 Train loss: 0.6705238 Test loss: 0.67673093\n",
      "100 Train loss: 0.0024167998 Test loss: 0.0011575341\n",
      "200 Train loss: 0.00019471091 Test loss: 0.00039884966\n",
      "300 Train loss: 0.00053543237 Test loss: 0.0001990199\n",
      "400 Train loss: 0.00032568534 Test loss: 0.00011874849\n",
      "500 Train loss: 0.00023515559 Test loss: 7.869401e-05\n",
      "600 Train loss: 1.5319756e-05 Test loss: 5.3570464e-05\n",
      "700 Train loss: 1.1277878e-05 Test loss: 3.9812243e-05\n",
      "800 Train loss: 5.028547e-05 Test loss: 3.0242772e-05\n",
      "900 Train loss: 3.908472e-05 Test loss: 2.3482637e-05\n",
      "1000 Train loss: 2.9880168e-06 Test loss: 1.8288581e-05\n",
      "1100 Train loss: 1.7669652e-06 Test loss: 1.4639259e-05\n",
      "1200 Train loss: 2.1956435e-05 Test loss: 1.1746229e-05\n",
      "1300 Train loss: 1.07126825e-05 Test loss: 9.463927e-06\n",
      "1400 Train loss: 6.8897916e-06 Test loss: 7.677755e-06\n",
      "1500 Train loss: 3.7250086e-06 Test loss: 6.3146417e-06\n",
      "1600 Train loss: 3.4760072e-07 Test loss: 5.0757008e-06\n",
      "1700 Train loss: 1.7157218e-07 Test loss: 4.1666904e-06\n",
      "1800 Train loss: 2.2580502e-06 Test loss: 3.3714493e-06\n",
      "1900 Train loss: 7.575916e-08 Test loss: 2.8601219e-06\n",
      "2000 Train loss: 8.0215585e-08 Test loss: 2.2910922e-06\n",
      "2100 Train loss: 5.9047583e-08 Test loss: 1.940298e-06\n",
      "2200 Train loss: 2.718386e-07 Test loss: 1.5818566e-06\n",
      "2300 Train loss: 4.0107793e-08 Test loss: 1.3068126e-06\n",
      "2400 Train loss: 1.7825686e-08 Test loss: 1.2515106e-06\n",
      "2500 Train loss: 7.798738e-09 Test loss: 9.1447293e-07\n",
      "2600 Train loss: 4.1221828e-08 Test loss: 7.5571756e-07\n",
      "2700 Train loss: 0.0 Test loss: 5.817003e-07\n",
      "2800 Train loss: 4.345002e-08 Test loss: 7.170998e-07\n",
      "2900 Train loss: 0.0 Test loss: 7.4999525e-07\n",
      "3000 Train loss: 0.0 Test loss: 5.788389e-07\n",
      "3100 Train loss: 7.798735e-09 Test loss: 4.7633225e-07\n",
      "3200 Train loss: 0.0 Test loss: 3.9051073e-07\n",
      "3300 Train loss: 6.684631e-09 Test loss: 3.194682e-07\n",
      "3400 Train loss: 0.0 Test loss: 2.6034468e-07\n",
      "3500 Train loss: 0.0 Test loss: 2.1314057e-07\n",
      "3600 Train loss: 0.0 Test loss: 1.7213453e-07\n",
      "3700 Train loss: 1.1141055e-09 Test loss: 1.3923405e-07\n",
      "3800 Train loss: 1.1141055e-09 Test loss: 1.1110153e-07\n",
      "3900 Train loss: 1.1141055e-09 Test loss: 9.2505346e-08\n",
      "4000 Train loss: 1.1141055e-09 Test loss: 8.106149e-08\n",
      "4100 Train loss: 0.0 Test loss: 7.152493e-08\n",
      "4200 Train loss: 0.0 Test loss: 5.817371e-08\n",
      "4300 Train loss: 0.0 Test loss: 4.4822443e-08\n",
      "4400 Train loss: 0.0 Test loss: 3.480896e-08\n",
      "4500 Train loss: 0.0 Test loss: 2.7656458e-08\n",
      "4600 Train loss: 0.0 Test loss: 2.2411283e-08\n",
      "4700 Train loss: 0.0 Test loss: 1.9073441e-08\n",
      "4800 Train loss: 0.0 Test loss: 1.621243e-08\n",
      "4900 Train loss: 0.0 Test loss: 1.430509e-08\n",
      "5000 Train loss: 0.0 Test loss: 1.2397747e-08\n",
      "5100 Train loss: 0.0 Test loss: 1.1444075e-08\n",
      "5200 Train loss: 0.0 Test loss: 1.0490404e-08\n",
      "5300 Train loss: 0.0 Test loss: 9.536732e-09\n",
      "5400 Train loss: 0.0 Test loss: 8.58306e-09\n",
      "5500 Train loss: 0.0 Test loss: 8.106223e-09\n",
      "5600 Train loss: 0.0 Test loss: 7.629387e-09\n",
      "5700 Train loss: 0.0 Test loss: 7.152551e-09\n",
      "5800 Train loss: 0.0 Test loss: 6.6757146e-09\n",
      "5900 Train loss: 0.0 Test loss: 6.198878e-09\n",
      "6000 Train loss: 0.0 Test loss: 6.198878e-09\n",
      "6100 Train loss: 0.0 Test loss: 5.722042e-09\n",
      "6200 Train loss: 0.0 Test loss: 5.245205e-09\n",
      "6300 Train loss: 0.0 Test loss: 5.245205e-09\n",
      "6400 Train loss: 0.0 Test loss: 4.768369e-09\n",
      "6500 Train loss: 0.0 Test loss: 4.768369e-09\n",
      "6600 Train loss: 0.0 Test loss: 4.768369e-09\n",
      "6700 Train loss: 0.0 Test loss: 4.291532e-09\n",
      "6800 Train loss: 0.0 Test loss: 4.291532e-09\n",
      "6900 Train loss: 0.0 Test loss: 4.291532e-09\n",
      "7000 Train loss: 0.0 Test loss: 3.8146952e-09\n",
      "7100 Train loss: 0.0 Test loss: 3.8146952e-09\n",
      "7200 Train loss: 0.0 Test loss: 3.8146952e-09\n",
      "7300 Train loss: 0.0 Test loss: 3.8146952e-09\n",
      "7400 Train loss: 0.0 Test loss: 3.3378587e-09\n",
      "7500 Train loss: 0.0 Test loss: 3.3378587e-09\n",
      "7600 Train loss: 0.0 Test loss: 3.3378587e-09\n",
      "7700 Train loss: 0.0 Test loss: 3.3378587e-09\n",
      "7800 Train loss: 0.0 Test loss: 3.3378587e-09\n",
      "7900 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8000 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8100 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8200 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8300 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8400 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8500 Train loss: 0.0 Test loss: 2.861022e-09\n",
      "8600 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "8700 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "8800 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "8900 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9000 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9100 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9200 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9300 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9400 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9500 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9600 Train loss: 0.0 Test loss: 2.384185e-09\n",
      "9700 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "9800 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "9900 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10000 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10100 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10200 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10300 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10400 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10500 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10600 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10700 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10800 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "10900 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "11000 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "11100 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "11200 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "11300 Train loss: 0.0 Test loss: 1.9073483e-09\n",
      "11400 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "11500 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "11600 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "11700 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "11800 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "11900 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12000 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12100 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12200 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12300 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12400 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12500 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12600 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12700 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12800 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "12900 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13000 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13100 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13200 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13300 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13400 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13500 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13600 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13700 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13800 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "13900 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "14000 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "14100 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "14200 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "14300 Train loss: 0.0 Test loss: 1.4305113e-09\n",
      "14400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "14500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "14600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "14700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "14800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "14900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "15900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "16900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17100 Train loss: 0.0 Test loss: 9.536743e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "17900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "18900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "19900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20700 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20800 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "20900 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21000 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21100 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21200 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21300 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21400 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21500 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21600 Train loss: 0.0 Test loss: 9.536743e-10\n",
      "21700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "21800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "21900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "22900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "23900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "24900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "25900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "26900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "27900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "28900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "29900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "30900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "31900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "32900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "33900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "34900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35000 Train loss: 0.0 Test loss: 4.768371e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "35900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "36900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "37900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "38900 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39000 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39100 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39200 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39300 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39400 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39500 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39600 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39700 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39800 Train loss: 0.0 Test loss: 4.768371e-10\n",
      "39900 Train loss: 0.0 Test loss: 4.768371e-10\n"
     ]
    }
   ],
   "source": [
    "print('d)')\n",
    "n_epochs = 40000\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_test = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, \"Train loss:\", loss_train, \"Test loss:\", loss_test)\n",
    "    save_path = saver.save(sess, \"./my_model_finalv2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_finalv2.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_finalv2.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: X_valid})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamOptimizer accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate and return the accuracy on the test data\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print('AdamOptimizer accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambos os otimizadores tem a maior preciso aps 40000 epochs, mas o Adam atinge um menor error em menos epochs que o Gradient Descent portanto o otimizador Adam tem melhor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
